{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a5940f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mkedw\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\mkedw\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\mkedw\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mkedw\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mkedw\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mkedw\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a4b116b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mkedw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1234c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kidhahania cream skimming ina vipimo viwili vy...</td>\n",
       "      <td>Bidhaa na jiografia ndizo hufanya skimming cre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unajua wakati wa msimu na nadhani kwa kiwango ...</td>\n",
       "      <td>Unapoteza vitu kwa kiwango kifuatacho ikiwa wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mmoja wa nambari zetu atatekeleza maagizo yako...</td>\n",
       "      <td>Mwanachama wa timu yangu atatekeleza maagizo y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unajuaje? Hii yote ni habari yao tena.</td>\n",
       "      <td>Habari hizi ni zao.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ndio nakuambia nini ingawa ukinunua baadhi ya ...</td>\n",
       "      <td>Viatu vya tenisi vina bei mbalimbali.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  Kidhahania cream skimming ina vipimo viwili vy...   \n",
       "1  unajua wakati wa msimu na nadhani kwa kiwango ...   \n",
       "2  Mmoja wa nambari zetu atatekeleza maagizo yako...   \n",
       "3             Unajuaje? Hii yote ni habari yao tena.   \n",
       "4  ndio nakuambia nini ingawa ukinunua baadhi ya ...   \n",
       "\n",
       "                                          hypothesis  label  \n",
       "0  Bidhaa na jiografia ndizo hufanya skimming cre...      1  \n",
       "1  Unapoteza vitu kwa kiwango kifuatacho ikiwa wa...      0  \n",
       "2  Mwanachama wa timu yangu atatekeleza maagizo y...      0  \n",
       "3                                Habari hizi ni zao.      0  \n",
       "4              Viatu vya tenisi vina bei mbalimbali.      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('swahili_train.csv')\n",
    "\n",
    "# Create a subset of 11,000 rows\n",
    "subset_df = df.head(9000).copy()\n",
    "\n",
    "# Display the first fe rows of the dataset\n",
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbc15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 'premise' column and the 'hypothesis' column into one column\n",
    "subset_df['combined_text'] = subset_df['premise'] + ' ' + subset_df['hypothesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c03de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your list of Swahili stopwords\n",
    "swahili_stopwords = [\n",
    "    \"akasema\", \"alikuwa\", \"alisema\", \"baada\", \"basi\", \"bila\", \"cha\", \"chini\",\n",
    "    \"hadi\", \"hapo\", \"hata\", \"hivyo\", \"hiyo\", \"huku\", \"huo\", \"ili\", \"ilikuwa\",\n",
    "    \"juu\", \"kama\", \"karibu\", \"katika\", \"kila\", \"kima\", \"kisha\", \"kubwa\", \"kutoka\",\n",
    "    \"kuwa\", \"kwa\", \"kwamba\", \"kwenda\", \"kwenye\", \"la\", \"lakini\", \"mara\", \"mdogo\",\n",
    "    \"mimi\", \"mkubwa\", \"mmoja\", \"moja\", \"muda\", \"mwenye\", \"na\", \"naye\", \"ndani\",\n",
    "    \"ng\", \"ni\", \"nini\", \"nonkungu\", \"pamoja\", \"pia\", \"sana\", \"sasa\", \"sauti\",\n",
    "    \"tafadhali\", \"tena\", \"tu\", \"vile\", \"wa\", \"wakati\", \"wake\", \"walikuwa\", \"wao\",\n",
    "    \"watu\", \"wengine\", \"wote\", \"ya\", \"yake\", \"yangu\", \"yao\", \"yeye\", \"yule\", \"za\", \"zaidi\", \"zake\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a638c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define a Swahili lemmatizer\n",
    "swahili_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for Swahili text cleaning\n",
    "def clean_swahili_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = [word.lower() for word in words if word not in string.punctuation]\n",
    "\n",
    "    # Remove Swahili stopwords\n",
    "    words = [word for word in words if word not in swahili_stopwords]\n",
    "\n",
    "    # Lemmatization (you may need to create a custom lemmatization function)\n",
    "    words = [swahili_lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "subset_df = subset_df.dropna(subset=['combined_text'])\n",
    "\n",
    "# Apply Swahili text cleaning to the 'combined_text' column\n",
    "subset_df['combined_text'] = subset_df['combined_text'].apply(lambda x: clean_swahili_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d26f04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    3331\n",
      "0    3078\n",
      "1    2589\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your dataset is stored in a DataFrame called 'subset_df' and the column with class labels is 'label'\n",
    "class_distribution = subset_df['label'].value_counts()\n",
    "\n",
    "# Display the class distribution\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dbfdf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Oversample classes 0 and 1\n",
    "class_0 = subset_df[subset_df['label'] == 0]\n",
    "class_1 = subset_df[subset_df['label'] == 1]\n",
    "class_2 = subset_df[subset_df['label'] == 2]\n",
    "\n",
    "oversampled_class_0 = resample(class_0, replace=True, n_samples=len(class_2), random_state=42)\n",
    "oversampled_class_1 = resample(class_1, replace=True, n_samples=len(class_2), random_state=42)\n",
    "\n",
    "oversampled_df = pd.concat([oversampled_class_0, oversampled_class_1, class_2])\n",
    "oversampled_df = oversampled_df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c6e2072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3331\n",
      "1    3331\n",
      "2    3331\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the class distribution in the oversampled dataset\n",
    "class_distribution = oversampled_df['label'].value_counts()\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3f2c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_text(text, tokenizer, max_length):\n",
    "    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    return tokens\n",
    "\n",
    "# Preprocess your data\n",
    "name = \"Davlan/bert-base-multilingual-cased-finetuned-swahili\"\n",
    "tokenizer = BertTokenizer.from_pretrained(name)\n",
    "max_length = 128  # You can adjust this based on your dataset\n",
    "\n",
    "X = oversampled_df['combined_text'].astype(str).tolist()  # Use the oversampled data\n",
    "y = oversampled_df['label']\n",
    "\n",
    "# Tokenize and split the data\n",
    "X_tokens = [tokenize_text(text, tokenizer, max_length) for text in X]\n",
    "X_tensors = torch.cat([t['input_ids'] for t in X_tokens], dim=0)\n",
    "attention_masks = torch.cat([t['attention_mask'] for t in X_tokens], dim=0)\n",
    "y_tensor = torch.tensor(y.values)\n",
    "\n",
    "# Calculate the minimum class count\n",
    "min_class_count = min(train_class_distribution)\n",
    "\n",
    "# Calculate the new test_size to maintain balance\n",
    "test_size = min_class_count / len(y)\n",
    "\n",
    "# Split your data into training and testing sets with the adjusted test_size\n",
    "X_train, X_test, y_train, y_test, train_masks, test_masks = train_test_split(\n",
    "    X_tensors, y_tensor, attention_masks, test_size=test_size, random_state=42, stratify=y_tensor\n",
    ")\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test, train_masks, test_masks = train_test_split(\n",
    "#    X_tensors, y_tensor, attention_masks, test_size=0.2, random_state=42, stratify=y_tensor\n",
    "# )\n",
    "\n",
    "# Define a PyTorch DataLoader for training and testing\n",
    "train_dataset_subset = TensorDataset(X_train, train_masks, y_train)\n",
    "test_dataset_subset = TensorDataset(X_test, test_masks, y_test)\n",
    "train_dataloader_subset = DataLoader(train_dataset_subset, batch_size=16, shuffle=True)\n",
    "test_dataloader_subset = DataLoader(test_dataset_subset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67e1968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Class Distribution:\n",
      "[2221 2221 2220]\n",
      "\n",
      "Testing Class Distribution:\n",
      "[1110 1110 1111]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert PyTorch tensors to NumPy arrays\n",
    "y_train_np = y_train.numpy()\n",
    "y_test_np = y_test.numpy()\n",
    "\n",
    "# Calculate the class distribution in the training set\n",
    "train_class_distribution = np.bincount(y_train_np)\n",
    "\n",
    "# Calculate the class distribution in the testing set\n",
    "test_class_distribution = np.bincount(y_test_np)\n",
    "\n",
    "# Display the class distribution\n",
    "print(\"Training Class Distribution:\")\n",
    "print(train_class_distribution)\n",
    "\n",
    "print(\"\\nTesting Class Distribution:\")\n",
    "print(test_class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c6ac432",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Davlan/bert-base-multilingual-cased-finetuned-swahili and are newly initialized: ['classifier.weight', 'bert.pooler.dense.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Model Selection and Loading\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load a pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('Davlan/bert-base-multilingual-cased-finetuned-swahili', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6091e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0610012429223643\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define class weights (adjust as needed)\n",
    "class_weights = [1.0, 1.0, 1.0]\n",
    "\n",
    "# Convert class weights to a tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Define a loss function (e.g., CrossEntropyLoss) with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Create an optimizer (e.g., Adam or Lamb)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader_subset, 0):\n",
    "        inputs, masks, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs, attention_mask=masks)\n",
    "        loss = criterion(outputs.logits, labels)  # Apply class weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / (i + 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e1b1e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     label_0       0.81      0.67      0.74       888\n",
      "     label_1       0.63      0.71      0.67       888\n",
      "     label_2       0.57      0.60      0.58       888\n",
      "\n",
      "    accuracy                           0.66      2664\n",
      "   macro avg       0.67      0.66      0.66      2664\n",
      "weighted avg       0.67      0.66      0.66      2664\n",
      "\n",
      "Confusion Matrix:\n",
      "[[597 116 175]\n",
      " [ 34 627 227]\n",
      " [105 252 531]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set your model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Iterate over the test data\n",
    "for batch in test_dataloader_subset:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "\n",
    "    # Move data to the device\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply softmax to get predicted probabilities\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    predicted_labels = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
    "\n",
    "    # Append to the lists\n",
    "    all_predictions.extend(predicted_labels)\n",
    "    all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(all_true_labels, all_predictions, target_names=['label_0', 'label_1', 'label_2'])\n",
    "print(report)\n",
    "\n",
    "# Calculate and print confusion matrix\n",
    "confusion = confusion_matrix(all_true_labels, all_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
